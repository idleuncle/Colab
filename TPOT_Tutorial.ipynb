{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPOT_Tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/idleuncle/Colab/blob/master/TPOT_Tutorial.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "RJziTwRVHfpz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TPOT Tutorial\n",
        "\n",
        "In this notebook, we will see how to use [TPOT](https://epistasislab.github.io/tpot/api/), a Python library developed for automatic machine learning feature preprocessing, model selection, and hyperparameter tuning. Using [genetic programming](http://geneticprogramming.com/tutorial/), TPOT tries to find the best machine learning pipeline for a dataset by evaluating thousands of possibilites.\n",
        "\n",
        "The machine learning pipeline in this context consists of:\n",
        "\n",
        "Feature Preprocessing\n",
        "Imputing missing values and scaling values\n",
        "Constructing new features such as polynomial transformations\n",
        "Feature selection\n",
        "Dimensionality reduction, for example using PCA and other techniques\n",
        "Model Selection\n",
        "Evaluting a number of machine learning models\n",
        "Hyperparameter tuning\n",
        "Finding the optimal settings of the model for the particular problem\n",
        "TPOT is one of a class of methods known as auto-ml (https://www.kdnuggets.com/2017/01/current-state-automated-machine-learning.html) which aim to simplify the work of the data scientist by automatically finding the optimal (or near-optimal) feature preprocessing steps and model for the problem. Machine learning is typically a very time-consuming and knowledge-intensive part of a data science problem. Auto-ml is not designed to replace the data scientist, but rather free her to work on more important aspects of the complete problem, such as acquiring data and interpreting the model results. In effect, TPOT, and auto-ml in general, will in effect be a \"data science assistant\" that will be another tool among many used by data scientists. Machine learning is only one part of the data science process, and it still takes a human to weave the different aspects of a problem together into a complete working product.\n",
        "\n",
        "Other entries in the field of auto - ml include:\n",
        "\n",
        "[Auto-sklearn](https://automl.github.io/auto-sklearn/stable/)\n",
        "\n",
        "[H20](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html)\n",
        "\n",
        "[Google Cloud AutoML](https://cloud.google.com/automl/)\n",
        "\n",
        "With that background, let's see how automated machine learning, the future of data science, works!\n"
      ]
    },
    {
      "metadata": {
        "id": "9TFiYMU9cxXz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 一、Colab 环境准备\n",
        "\n",
        "1. 在Colab中打开ColabTemplate.ipynb，另存为你的项目MyProject.ipynb并打开。\n",
        "\n",
        "2. \"Colab 环境准备\" 完成以下工作，只需要项目打开 时执行一次。\n",
        "\n",
        "    - 安装系统依赖\n",
        "\n",
        "    - 授权登录 Google Drive\n",
        "\n",
        "    - 安装 Colab 编程环境支持包 (IpynbImporter.py, [ColabModules.ipynb](https://colab.research.google.com/drive/1IMv93f2bMYhrx2lfL3cmDBI7kmjCMy01#scrollTo=VyDM84dOxu18))\n",
        "    \n",
        "3. 修改并保存ColabModules.ipynb后，执行“下载 Colab 编程环境支持包”及“导入 Colab 编程环境支持包”。\n",
        "\n",
        "drive变量指向登录的Google Drive。"
      ]
    },
    {
      "metadata": {
        "id": "jIZcTrsFVJw0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1 授权登录Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "UmVckKTzISJZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 第一次授权登录Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "dTLcxx_AVNo0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 安装 PyDrive 操作库，该操作每个 notebook 只需要执行一次\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "def google_drive_login():\n",
        "  # 授权登录，仅第一次的时候会鉴权\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  return drive\n",
        "\n",
        "drive = google_drive_login()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Shy_-Y2IaTT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 第二次授权映射Google Drive至本地driver目录"
      ]
    },
    {
      "metadata": {
        "id": "6K-Lqa-AnaF5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "creds = GoogleCredentials.get_application_default()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wvd9DiXvojev",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MWRoI2tlodDr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "!ls -lt drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GAsd62uhVV9g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.2 下载 Colab 编程环境支持包"
      ]
    },
    {
      "metadata": {
        "id": "ufRo_PUAVW7x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "def google_drive_download_files(drive, file_name_prefix, colab_dir=\".\", overwrite=True):\n",
        "  # choose a local (colab) directory to store the data.\n",
        "  local_download_path = os.path.expanduser(colab_dir)\n",
        "  try:\n",
        "    os.makedirs(local_download_path)\n",
        "  except: pass\n",
        "\n",
        "  # 2. Auto-iterate using the query syntax\n",
        "  #    https://developers.google.com/drive/v2/web/search-parameters\n",
        "  file_list = drive.ListFile(\n",
        "      {'q': \"title contains '%s'\" % (file_name_prefix) }).GetList()\n",
        "\n",
        "  files_dict = {}\n",
        "  for f in file_list:\n",
        "    # 3. Create & download by id.\n",
        "    print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "    fname = os.path.join(local_download_path, f['title'])\n",
        "    if overwrite or not os.path.exists(fname):\n",
        "      print('downloading to {}'.format(fname))\n",
        "      f_ = drive.CreateFile({'id': f['id']})\n",
        "      f_.GetContentFile(fname)\n",
        "      print('Download Completed!')\n",
        "    files_dict[ f['title'] ] = fname\n",
        "\n",
        "  # with open(fname, 'r') as f:\n",
        "  #   print(f.read())\n",
        "  return files_dict, local_download_path\n",
        "\n",
        "# 修改完ColabModles.ipynb后，执行以下命令，(并在项目中执行菜单项 Runtime/Restart runtime ???)\n",
        "google_drive_download_files(drive, 'IpynbImporter.py')\n",
        "google_drive_download_files(drive, 'ColabModules.ipynb')\n",
        "\n",
        "!ls -lt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oha223wVcarf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3 导入Colab基础编程环境支持包"
      ]
    },
    {
      "metadata": {
        "id": "VAZFrZ1acWh5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import IpynbImporter\n",
        "from ColabModules import *\n",
        "\n",
        "colab_ready()\n",
        "\n",
        "!hostname\n",
        "!ls -lt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wi1hQH26I6Or",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 二、开始研究代码\n",
        "\n",
        "The task is a supervised regression problem: given [New York City energy data](http://www.nyc.gov/html/gbee/html/plan/ll84_scores.shtml), we want to build a model that can predict the Energy Star Score of a building. In a series of articles ([part one](https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420), [part two](https://towardsdatascience.com/a-complete-machine-learning-project-walk-through-in-python-part-two-300f1f8147e2), [part three,](https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-three-388834e8804b) [code on GitHub](https://github.com/WillKoehrsen/machine-learning-project-walkthrough)), we built a complete machine learning solution for this problem. Using manual feature engineering, dimensionality reduction, model selection, and hyperparameter tuning, we were able to build a model that achieved a mean absolute error of 9.06 points (on a scale of 1-100) on the test set."
      ]
    },
    {
      "metadata": {
        "id": "JIlKt6kQVqhL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install tpot on the server\n",
        "!pip install tpot\n",
        "\n",
        "# pandas and numpy for data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import the tpot regressor\n",
        "from tpot import TPOTRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xoljM8R9OumX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 数据集\n",
        "The features contain a number of continuous numeric variables (such as energy use and area of the building) as well as two one-hot encoded categorical variables (borough and building type). There are a total of 82 features.\n",
        "\n",
        "All of the missing values have been encoded as np.nan, and TPOT will automatically perform missing value imputation. It also automatically scales the variables so we do not have to worry about normalizing the range of each feature. TPOT does both feature engineering and feature selection, so we will not transform any of the variables or remove extraneous features we think may be extraneous.\n",
        "\n",
        "We will read into the data from GitHub and take a brief look."
      ]
    },
    {
      "metadata": {
        "id": "mBT1FaWDNuRK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "dd5b71aa-b05d-46d5-dd9e-1867cb0326a3"
      },
      "cell_type": "code",
      "source": [
        "# Read in features from GitHub\n",
        "train_features = pd.read_csv('https://raw.githubusercontent.com/WillKoehrsen/machine-learning-project-walkthrough/master/data/X_train.csv')\n",
        "test_features = pd.read_csv('https://raw.githubusercontent.com/WillKoehrsen/machine-learning-project-walkthrough/master/data/X_test.csv')\n",
        "\n",
        "# Read in labels from GitHub\n",
        "train_labels = pd.read_csv('https://raw.githubusercontent.com/WillKoehrsen/machine-learning-project-walkthrough/master/data/Y_train.csv')\n",
        "test_labels = pd.read_csv('https://raw.githubusercontent.com/WillKoehrsen/machine-learning-project-walkthrough/master/data/Y_test.csv')\n",
        "\n",
        "print('Training features shape: ', train_features.shape)\n",
        "print('Testing features shape:  ', test_features.shape)\n",
        "\n",
        "train_features.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training features shape:  (6622, 82)\n",
            "Testing features shape:   (2839, 82)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Order</th>\n",
              "      <th>Property Id</th>\n",
              "      <th>DOF Gross Floor Area</th>\n",
              "      <th>Largest Property Use Type - Gross Floor Area (ft²)</th>\n",
              "      <th>Year Built</th>\n",
              "      <th>Number of Buildings - Self-reported</th>\n",
              "      <th>Occupancy</th>\n",
              "      <th>Site EUI (kBtu/ft²)</th>\n",
              "      <th>Weather Normalized Site EUI (kBtu/ft²)</th>\n",
              "      <th>Weather Normalized Site Electricity Intensity (kWh/ft²)</th>\n",
              "      <th>...</th>\n",
              "      <th>Largest Property Use Type_Restaurant</th>\n",
              "      <th>Largest Property Use Type_Retail Store</th>\n",
              "      <th>Largest Property Use Type_Self-Storage Facility</th>\n",
              "      <th>Largest Property Use Type_Senior Care Community</th>\n",
              "      <th>Largest Property Use Type_Social/Meeting Hall</th>\n",
              "      <th>Largest Property Use Type_Strip Mall</th>\n",
              "      <th>Largest Property Use Type_Supermarket/Grocery Store</th>\n",
              "      <th>Largest Property Use Type_Urgent Care/Clinic/Other Outpatient</th>\n",
              "      <th>Largest Property Use Type_Wholesale Club/Supercenter</th>\n",
              "      <th>Largest Property Use Type_Worship Facility</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>13276</td>\n",
              "      <td>5849784</td>\n",
              "      <td>90300.0</td>\n",
              "      <td>77300.0</td>\n",
              "      <td>1950</td>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>126.0</td>\n",
              "      <td>136.8</td>\n",
              "      <td>5.2</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7377</td>\n",
              "      <td>4398442</td>\n",
              "      <td>52000.0</td>\n",
              "      <td>52000.0</td>\n",
              "      <td>1926</td>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>95.4</td>\n",
              "      <td>102.0</td>\n",
              "      <td>4.7</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9479</td>\n",
              "      <td>4665374</td>\n",
              "      <td>104700.0</td>\n",
              "      <td>105000.0</td>\n",
              "      <td>1954</td>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>40.4</td>\n",
              "      <td>40.0</td>\n",
              "      <td>3.8</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14774</td>\n",
              "      <td>3393340</td>\n",
              "      <td>129333.0</td>\n",
              "      <td>129333.0</td>\n",
              "      <td>1992</td>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>157.1</td>\n",
              "      <td>163.1</td>\n",
              "      <td>16.9</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3286</td>\n",
              "      <td>2704325</td>\n",
              "      <td>109896.0</td>\n",
              "      <td>116041.0</td>\n",
              "      <td>1927</td>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>62.3</td>\n",
              "      <td>68.2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 82 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Order  Property Id  DOF Gross Floor Area  \\\n",
              "0  13276      5849784               90300.0   \n",
              "1   7377      4398442               52000.0   \n",
              "2   9479      4665374              104700.0   \n",
              "3  14774      3393340              129333.0   \n",
              "4   3286      2704325              109896.0   \n",
              "\n",
              "   Largest Property Use Type - Gross Floor Area (ft²)  Year Built  \\\n",
              "0                                            77300.0         1950   \n",
              "1                                            52000.0         1926   \n",
              "2                                           105000.0         1954   \n",
              "3                                           129333.0         1992   \n",
              "4                                           116041.0         1927   \n",
              "\n",
              "   Number of Buildings - Self-reported  Occupancy  Site EUI (kBtu/ft²)  \\\n",
              "0                                    1        100                126.0   \n",
              "1                                    1        100                 95.4   \n",
              "2                                    1        100                 40.4   \n",
              "3                                    1        100                157.1   \n",
              "4                                    1        100                 62.3   \n",
              "\n",
              "   Weather Normalized Site EUI (kBtu/ft²)  \\\n",
              "0                                   136.8   \n",
              "1                                   102.0   \n",
              "2                                    40.0   \n",
              "3                                   163.1   \n",
              "4                                    68.2   \n",
              "\n",
              "   Weather Normalized Site Electricity Intensity (kWh/ft²)  \\\n",
              "0                                                5.2         \n",
              "1                                                4.7         \n",
              "2                                                3.8         \n",
              "3                                               16.9         \n",
              "4                                                3.5         \n",
              "\n",
              "                      ...                      \\\n",
              "0                     ...                       \n",
              "1                     ...                       \n",
              "2                     ...                       \n",
              "3                     ...                       \n",
              "4                     ...                       \n",
              "\n",
              "   Largest Property Use Type_Restaurant  \\\n",
              "0                                     0   \n",
              "1                                     0   \n",
              "2                                     0   \n",
              "3                                     0   \n",
              "4                                     0   \n",
              "\n",
              "   Largest Property Use Type_Retail Store  \\\n",
              "0                                       0   \n",
              "1                                       0   \n",
              "2                                       0   \n",
              "3                                       0   \n",
              "4                                       0   \n",
              "\n",
              "   Largest Property Use Type_Self-Storage Facility  \\\n",
              "0                                                0   \n",
              "1                                                0   \n",
              "2                                                0   \n",
              "3                                                0   \n",
              "4                                                0   \n",
              "\n",
              "   Largest Property Use Type_Senior Care Community  \\\n",
              "0                                                0   \n",
              "1                                                0   \n",
              "2                                                0   \n",
              "3                                                1   \n",
              "4                                                0   \n",
              "\n",
              "   Largest Property Use Type_Social/Meeting Hall  \\\n",
              "0                                              0   \n",
              "1                                              0   \n",
              "2                                              0   \n",
              "3                                              0   \n",
              "4                                              0   \n",
              "\n",
              "   Largest Property Use Type_Strip Mall  \\\n",
              "0                                     0   \n",
              "1                                     0   \n",
              "2                                     0   \n",
              "3                                     0   \n",
              "4                                     0   \n",
              "\n",
              "   Largest Property Use Type_Supermarket/Grocery Store  \\\n",
              "0                                                  0     \n",
              "1                                                  0     \n",
              "2                                                  0     \n",
              "3                                                  0     \n",
              "4                                                  0     \n",
              "\n",
              "   Largest Property Use Type_Urgent Care/Clinic/Other Outpatient  \\\n",
              "0                                                  0               \n",
              "1                                                  0               \n",
              "2                                                  0               \n",
              "3                                                  0               \n",
              "4                                                  0               \n",
              "\n",
              "   Largest Property Use Type_Wholesale Club/Supercenter  \\\n",
              "0                                                  0      \n",
              "1                                                  0      \n",
              "2                                                  0      \n",
              "3                                                  0      \n",
              "4                                                  0      \n",
              "\n",
              "   Largest Property Use Type_Worship Facility  \n",
              "0                                           0  \n",
              "1                                           0  \n",
              "2                                           0  \n",
              "3                                           0  \n",
              "4                                           0  \n",
              "\n",
              "[5 rows x 82 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "OJnM2IdKN235",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Convert to numpy arrays\n",
        "training_features = np.array(train_features)\n",
        "testing_features = np.array(test_features)\n",
        "\n",
        "# Sklearn wants the labels as one-dimensional vectors\n",
        "training_targets = np.array(train_labels).reshape((-1,))\n",
        "testing_targets = np.array(test_labels).reshape((-1,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b-WvoeusO7Uk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 TPOT优化器\n",
        "After the minimal data preparation, we can create the TPOT optimizer. The syntax for TPOT optimizers is designed to be as close to that for Scikit-Learn models as possible.\n",
        "\n",
        "The default parameters for TPOT optimizers will test 100 populations of pipelines, each with 100 generations for a total of 10,000 pipelines. Using 10-fold cross validation, this represents 100,000 training runs. Even using Google Colab, this takes quite a while! To avoid running out of time on the Colab server (we get 12 hours of continuous run-time) we will set a maximum of 8 hours (480 minutes) for evaluation. TPOT is designed to be run for days to thoroughly evaluate many pipelines, but the results can be quite good even from a few hours of training.\n",
        "\n",
        "We set the following parameters in the call to the optimizer (feel free to change these and see how they affect the results):\n",
        "\n",
        "scoring = neg_mean_absolute_error: Our selected regression performance metric\n",
        "max_time_mins = 480: Limit evaluation to 8 hours\n",
        "n_jobs = -1: Use all available cores on the machine\n",
        "verbosity = 2: Show a limited amount of information while training\n",
        "cv = 5: Use 5-fold cross validation (default is 10)\n",
        "After we create the optimizer, we fit it to the training data as with any Scikit-Learn machine learning model. This starts the optimization process which will continue for 8 hours. During training, we can see a limited amount of information (change the verbosity to see more or less).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hnvHawO2N9yA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a tpot object with a few parameters\n",
        "tpot = TPOTRegressor(scoring = 'neg_mean_absolute_error', \n",
        "                    max_time_mins = 480, \n",
        "                    n_jobs = -1,\n",
        "                    verbosity = 2,\n",
        "                    cv = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XerBkrZ8OBxh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Fit the tpot model on the training data\n",
        "tpot.fit(training_features, training_targets)\n",
        "\n",
        "# Show the final model\n",
        "print(tpot.fitted_pipeline_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2rJ2GaccSDIE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Due to the time limit, we can see our model only was able to get through 15 generations. With 100 populations, this represents 1500 different individual pipelines that were evaluated, quit a few more than we would be able to try by hand!\n",
        "\n",
        "Once the model has finished training, we can see the optimal pipeline by printing the fitted_pipeline. This represents the complete pipeline with the best performance metric (in this case the highest neg_mean_absolute_error) from cross validation."
      ]
    },
    {
      "metadata": {
        "id": "XYymaqkqPB5e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2.2.1 导出最佳模型"
      ]
    },
    {
      "metadata": {
        "id": "zfrYkfWsOO0w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Export the pipeline as a python script file\n",
        "tpot.export('tpot_exported_pipeline.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XzkRp74cOSYH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import file management\n",
        "from google.colab import file\n",
        "\n",
        "# Download the pipeline for local use\n",
        "files.download('tpot_exported_pipeline.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TnDnrwfYPI4Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2.2.2 查看优选指标"
      ]
    },
    {
      "metadata": {
        "id": "LE2Sh1j3OWUR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# To examine all fitted models\n",
        "# tpot.evaluated_individuals_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "liyh1ng9Ok3I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3 在测试集上验证最佳模型"
      ]
    },
    {
      "metadata": {
        "id": "wV5kw7zlOa1o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Imports that the final pipeline needs\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import LassoLarsCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline, make_union\n",
        "from sklearn.preprocessing import Imputer\n",
        "from tpot.builtins import StackingEstimator\n",
        "\n",
        "# Preprocessing steps\n",
        "imputer = Imputer(strategy=\"median\")\n",
        "imputer.fit(training_features)\n",
        "training_features = imputer.transform(training_features)\n",
        "testing_features = imputer.transform(testing_features)\n",
        "\n",
        "# Final pipeline from TPOT\n",
        "exported_pipeline = make_pipeline(\n",
        "    StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n",
        "    GradientBoostingRegressor(alpha=0.95, learning_rate=0.1, loss=\"lad\", \n",
        "                              max_depth=7, max_features=0.75, \n",
        "                              min_samples_leaf=3, min_samples_split=18, \n",
        "                              n_estimators=100, subsample=0.60)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QYO2Dqn8OesN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Fit on the training data\n",
        "exported_pipeline.fit(training_features, training_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Sim-CvOOh9k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Make predictions on the testing data\n",
        "predictions = exported_pipeline.predict(testing_features)\n",
        "\n",
        "print('Mean Absolute Error = %0.4f' % np.mean(abs(predictions - testing_targets)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}